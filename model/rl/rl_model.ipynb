{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbenip/tetris-ai/blob/main/model/rl/rl_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0x51aWmuNJN",
        "outputId": "6ec07998-b864-47d2-c149-dc2d6cc508d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bbenip/tetris-ai.git tetris-ai\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI6aPENMuR89",
        "outputId": "4da4a8db-88b9-4a84-c936-6672574090ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tetris-ai'...\n",
            "remote: Enumerating objects: 202, done.\u001b[K\n",
            "remote: Counting objects: 100% (109/109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 202 (delta 40), reused 44 (delta 11), pack-reused 93\u001b[K\n",
            "Receiving objects: 100% (202/202), 36.55 MiB | 31.75 MiB/s, done.\n",
            "Resolving deltas: 100% (72/72), done.\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mtetris-ai\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd tetris-ai/model/rl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxSFnb41UiPX",
        "outputId": "d5a19894-823c-464a-bfe2-3539930b8542"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tetris-ai/model/rl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShQ3oTxSGgv9",
        "outputId": "27c290a5-3877-4a38-8899-6d9821299d72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects:  33% (1/3)\u001b[K\rremote: Compressing objects:  66% (2/3)\u001b[K\rremote: Compressing objects: 100% (3/3)\u001b[K\rremote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 5 (delta 2), reused 5 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  20% (1/5)\rUnpacking objects:  40% (2/5)\rUnpacking objects:  60% (3/5)\rUnpacking objects:  80% (4/5)\rUnpacking objects: 100% (5/5)\rUnpacking objects: 100% (5/5), 2.29 KiB | 1.15 MiB/s, done.\n",
            "From https://github.com/bbenip/tetris-ai\n",
            " + d536baa...431c55b main       -> origin/main  (forced update)\n",
            "\u001b[33mhint: You have divergent branches and need to specify how to reconcile them.\u001b[m\n",
            "\u001b[33mhint: You can do so by running one of the following commands sometime before\u001b[m\n",
            "\u001b[33mhint: your next pull:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint:   git config pull.rebase false  # merge (the default strategy)\u001b[m\n",
            "\u001b[33mhint:   git config pull.rebase true   # rebase\u001b[m\n",
            "\u001b[33mhint:   git config pull.ff only       # fast-forward only\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: You can replace \"git config\" with \"git config --global\" to set a default\u001b[m\n",
            "\u001b[33mhint: preference for all repositories. You can also pass --rebase, --no-rebase,\u001b[m\n",
            "\u001b[33mhint: or --ff-only on the command line to override the configured default per\u001b[m\n",
            "\u001b[33mhint: invocation.\u001b[m\n",
            "fatal: Need to specify how to reconcile divergent branches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Constants\n",
        "NUM_EPISODES = 2048 # @param {type:\"integer\"}\n",
        "MAX_MOVES = 1024 # @param {type:\"integer\"}\n",
        "BATCH_SIZE = 1024 # @param {type:\"integer\"}\n",
        "DISCOUNT_FACTOR = 1 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "EXPLORATION_START = 0.3 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "EXPLORATION_END = 0.3 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "E_DECAY_STEPS = 2048 # @param {type:\"integer\"}\n",
        "LEARNING_RATE = 0.001 # @param {type:\"number\"}\n",
        "\n",
        "EPISODES_PER_UPDATE = 150 # @param {type:\"integer\"}\n",
        "\n",
        "INVALID_MOVE_REWARD = -999 # @param {type:\"integer\"}\n",
        "GAME_OVER_REWARD = -999 # @param {type:\"integer\"}\n",
        "VALID_MOVE_REWARD = 1 # @param {type:\"integer\"}\n",
        "\n",
        "#Amount subtracted from reward for every increase in line height\n",
        "LINE_HEIGHT_PENALTY = 10 # @param {type:\"integer\"}\n"
      ],
      "metadata": {
        "id": "TKAQEhkBwb5v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate, InputLayer"
      ],
      "metadata": {
        "id": "HsTnyAb0KgQU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  # Define the input for the CNN branch\n",
        "  board_input = Input(shape=(21, 10, 1), name='board_input')\n",
        "  current_piece_input = Input(shape=(4, 4, 1), name='current_piece_input')\n",
        "  next_piece_input = Input(shape=(4, 4, 1), name='next_piece_input')\n",
        "  position_input = Input(shape=(2), name=\"position_input\")\n",
        "  line_height_input = Input(shape=(1), name=\"line_height_input\")\n",
        "\n",
        "  # Create the CNN branch\n",
        "  board_branch = Conv2D(256, (3, 3), activation='relu')(board_input)\n",
        "  board_branch = MaxPooling2D((2, 2), padding='same')(board_branch)\n",
        "  board_branch = Conv2D(32, (3, 3), activation='relu')(board_branch)\n",
        "  board_branch = MaxPooling2D((2, 2), padding='same')(board_branch)\n",
        "  board_branch = Flatten()(board_branch)\n",
        "\n",
        "  current_branch = Conv2D(32, (3, 3), activation='relu')(current_piece_input)\n",
        "  current_branch = Flatten()(current_branch)\n",
        "\n",
        "  next_branch = Conv2D(32, (3, 3), activation='relu')(next_piece_input)\n",
        "  next_branch = Flatten()(next_branch)\n",
        "\n",
        "  # Concatenate the 3 branches + position\n",
        "  concatenated_inputs = concatenate([board_branch,\n",
        "                                     current_branch,\n",
        "                                     next_branch,\n",
        "                                     position_input,\n",
        "                                     line_height_input],\n",
        "                                    name='concatenated_inputs')\n",
        "\n",
        "  # Create the dense branch for the concatenated inputs\n",
        "  dense_branch = Dense(64, activation='relu')(concatenated_inputs)\n",
        "  dense_branch = Dense(64, activation='relu')(dense_branch)\n",
        "  dense_branch = Dense(32, activation='relu')(dense_branch)\n",
        "  dense_branch = Dense(32, activation='relu')(dense_branch)\n",
        "\n",
        "  # Create the output layer\n",
        "  output = Dense(6, activation='linear', name='output')(dense_branch)\n",
        "\n",
        "  # Define the model with 4 inputs and one output\n",
        "  model = tf.keras.Model(inputs=[board_input,\n",
        "                                 current_piece_input,\n",
        "                                 next_piece_input,\n",
        "                                 position_input,\n",
        "                                 line_height_input],\n",
        "                         outputs=output)\n",
        "\n",
        "  # Define the optimizer and loss function based on your RL task.\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)  # Adjust the learning rate as needed.\n",
        "  loss_fn = tf.keras.losses.MeanSquaredError()  # Adjust the loss function as needed.\n",
        "\n",
        "  # Compile the model.\n",
        "  model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "fW9yNaju1nXJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]"
      ],
      "metadata": {
        "id": "num5EkHMNZ3u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def dqn(env,\n",
        "        num_episodes,\n",
        "        pretrained_model=None,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        gamma=DISCOUNT_FACTOR,\n",
        "        ε_start=EXPLORATION_START,\n",
        "        ε_end=EXPLORATION_END,\n",
        "        ε_decay_steps=E_DECAY_STEPS,\n",
        "        learning_rate=LEARNING_RATE):\n",
        "    replay_buffer = ReplayMemory(max_size=10000)\n",
        "    model = pretrained_model if pretrained_model else create_model()  # Define the DQN neural network.\n",
        "    target_model =  pretrained_model if pretrained_model else create_model()  # Target network for stability.\n",
        "\n",
        "    current_step = 0\n",
        "    scores = np.zeros(NUM_EPISODES)\n",
        "    move_counts = np.zeros(NUM_EPISODES)\n",
        "    avg_past_ten = np.zeros(NUM_EPISODES // EPISODES_PER_UPDATE + 1)\n",
        "\n",
        "    for episode in range(NUM_EPISODES):\n",
        "        score = 0\n",
        "        moves = 0\n",
        "        count = 0\n",
        "\n",
        "        env.start_game()\n",
        "        state = env.getState()\n",
        "        done = False\n",
        "        while not done and moves < MAX_MOVES:\n",
        "          if current_step < ε_decay_steps:\n",
        "            # Update epsilon using linear decay\n",
        "            epsilon = max(ε_end, ε_start - (ε_start - ε_end) * (current_step / ε_decay_steps))\n",
        "\n",
        "          # Epsilon-greedy action selection\n",
        "          if np.random.rand() < epsilon:\n",
        "              action = np.random.randint(6)  # Explore\n",
        "              #print(f\"Random Action: {action}\")\n",
        "          else:\n",
        "              model_input = state\n",
        "              model_input[0] = tf.expand_dims(tf.expand_dims(test, 0), -1)\n",
        "              model_input[1] = tf.expand_dims(model_input[1], 0)\n",
        "              model_input[2] = tf.expand_dims(model_input[2], 0)\n",
        "              action = np.argmax(model(model_input))\n",
        "              #print(f\"Action: {action}\")\n",
        "\n",
        "          next_state, reward, done = env.doAction(action)\n",
        "          score += reward if reward > 0 else 0\n",
        "          replay_buffer.append((state, action, reward, next_state, done))\n",
        "          state = next_state\n",
        "\n",
        "          count += 1\n",
        "          moves += 1\n",
        "          # Sample and train on mini-batch from replay buffer\n",
        "          if replay_buffer.size >= batch_size and count > batch_size:\n",
        "              count = 0\n",
        "              mini_batch = replay_buffer.sample(batch_size)\n",
        "              train_dqn(model, target_model, mini_batch, gamma, learning_rate)\n",
        "        current_step += 1\n",
        "        scores[episode] = score\n",
        "        move_counts[episode] = moves\n",
        "        avg_past_ten[episode // EPISODES_PER_UPDATE] = np.average(scores[episode-10:episode])\n",
        "        if episode and episode % EPISODES_PER_UPDATE == 0:\n",
        "            #clear_output(wait=True)\n",
        "            print(f\"Episode: {episode} \\nAverage Score (Past 10 games): {avg_past_ten[episode // EPISODES_PER_UPDATE]}\")\n",
        "            fig, ax1 = plt.subplots()\n",
        "\n",
        "            color = 'tab:red'\n",
        "            ax1.set_xlabel('Episode')\n",
        "            ax1.set_ylabel('Score', color=color)\n",
        "            ax1.scatter(range(episode), scores[:episode], color=color)\n",
        "            ax1.plot(range(0, episode, EPISODES_PER_UPDATE), avg_past_ten[:episode // EPISODES_PER_UPDATE], color=\"black\", linewidth=2, label='moving_avg')\n",
        "            ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "            ax2 = plt.subplot(312, sharex=ax1)\n",
        "            ax2.plot(range(episode), move_counts[:episode], color=\"blue\", label=\"score\")\n",
        "\n",
        "            fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "            plt.show()\n",
        "\n",
        "        if episode % (EPISODES_PER_UPDATE * 2) == 0:\n",
        "            model.save(\"./model\")\n"
      ],
      "metadata": {
        "id": "fGJLD5DL543d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_dqn(model, target_model, mini_batch, gamma, learning_rate):\n",
        "    # Step 1: Compute the target Q-values using the target model\n",
        "    states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
        "\n",
        "    target_q_values = []\n",
        "    states_iter = zip(next_states, rewards, dones)\n",
        "    for next_state, reward, done in states_iter:\n",
        "        if done:\n",
        "            target_q_values.append(rewards)  # If it's a terminal state, the Q-value is the immediate reward.\n",
        "        else:\n",
        "            # Use the target model to predict Q-values for the next state and select the maximum Q-value.\n",
        "            next_state[0] = tf.expand_dims(tf.expand_dims(next_state[0], 0), -1)\n",
        "            next_state[1] = tf.expand_dims(tf.expand_dims(next_state[1], 0), -1)\n",
        "            next_state[2] = tf.expand_dims(tf.expand_dims(next_state[2], 0), -1)\n",
        "            max_q_value = np.max(model(next_state)[0])\n",
        "            target_q = rewards + gamma * max_q_value\n",
        "            target_q_values.append(target_q)\n",
        "\n",
        "    target_q_values = np.array(target_q_values)\n",
        "\n",
        "    # Step 2: Compute the predicted Q-values for the current states using the model\n",
        "    model_inputs = np.dstack(states)\n",
        "    print(\"Curr pieces \", model_inputs[1].shape)\n",
        "    print(\"Boards \", model_inputs[0].shape)\n",
        "    print(\"Targets \", target_q_values.shape)\n",
        "\n",
        "    # Step 3: Calculate the target Q-values for the mini-batch\n",
        "    target_q_values = np.expand_dims(target_q_values, axis=1)  # Add an extra dimension for broadcasting\n",
        "\n",
        "    # Step 5: Update the model's weights using gradient descent to minimize the loss\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    predicted_q_values = model(model_inputs)\n",
        "    with tf.GradientTape() as tape:\n",
        "        #predicted_q_values = tf.reduce_sum(tf.multiply(predicted_q_values, masks), axis=1)\n",
        "        loss = tf.keras.losses.mean_squared_error(target_q_values, predicted_q_values)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss.numpy().mean()\n"
      ],
      "metadata": {
        "id": "3-EI4jNw_k7I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_model = None"
      ],
      "metadata": {
        "id": "gflapR-uw0N0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_model = tf.keras.models.load_model(\"./model\")"
      ],
      "metadata": {
        "id": "feMekBrmvjY2",
        "outputId": "1b90a2f5-2e84-49d9-8088-de717edfa328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-59821eeb18e0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpre_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at ./model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tetris import TetrisApp\n",
        "\n",
        "REWARDS = (INVALID_MOVE_REWARD, GAME_OVER_REWARD, VALID_MOVE_REWARD, LINE_HEIGHT_PENALTY)\n",
        "env = TetrisApp(ai=True, rewards=REWARDS)\n",
        "dqn(env,\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    pretrained_model=pre_model,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    gamma=DISCOUNT_FACTOR,\n",
        "    learning_rate=LEARNING_RATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "id": "yUXUGuzI4V3S",
        "outputId": "92a159c8-1ff6-4fcf-9d08-a3f9127ce95e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8377f5ade845>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mREWARDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mINVALID_MOVE_REWARD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAME_OVER_REWARD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALID_MOVE_REWARD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLINE_HEIGHT_PENALTY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTetrisApp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREWARDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m dqn(env,\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpretrained_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2a148aa6073a>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(env, num_episodes, pretrained_model, batch_size, gamma, ε_start, ε_end, ε_decay_steps, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmoves\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mMAX_MOVES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tetris-ai/model/rl/tetris.py\u001b[0m in \u001b[0;36mgetState\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrView\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mnext_piece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadToShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_stone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#2x2, 2x3, 1x4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mcurrent_piece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadToShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#2x2, 2x3, 1x4 padded to 4x4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstone_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstone_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#1x2 #2x1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mline_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tetris-ai/model/rl/tetris.py\u001b[0m in \u001b[0;36mtf__padToShape\u001b[0;34m(arr, shape, offset)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mp_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mpaddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_top\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_bottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/content/tetris-ai/model/rl/tetris.py\", line 147, in padToShape  *\n        padded = tf.pad(arr, paddings)\n\n    ValueError: Paddings must be non-negative for '{{node Pad}} = Pad[T=DT_INT32, Tpaddings=DT_INT32](arr, Const)' with input shapes: [1,10], [2,2] and with computed input tensors: input[1] = <[0 3][0 -6]>.\n"
          ]
        }
      ]
    }
  ]
}