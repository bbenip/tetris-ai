{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbenip/tetris-ai/blob/main/model/rl/rl_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S0x51aWmuNJN",
        "outputId": "782ea213-977b-41d2-e852-b510b3e83047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bbenip/tetris-ai.git tetris-ai\n",
        "%cd tetris-ai/model/rl\n",
        "%ls"
      ],
      "metadata": {
        "id": "yI6aPENMuR89",
        "outputId": "12a1a5ac-47b0-4546-d2d7-ff3c4e1333a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tetris-ai'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 152 (delta 22), reused 20 (delta 4), pack-reused 93\u001b[K\n",
            "Receiving objects: 100% (152/152), 36.43 MiB | 31.72 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "/content/tetris-ai/model/rl/tetris-ai/model/rl\n",
            "rl_model.ipynb  tetris.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "ShQ3oTxSGgv9",
        "outputId": "b9d6e1c7-8e3e-4250-9793-bb8f0cb0dc9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate, InputLayer"
      ],
      "metadata": {
        "id": "HsTnyAb0KgQU"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  # Define the input for the CNN branch\n",
        "  board_input = Input(shape=(21, 10, 1), name='board_input')\n",
        "  current_piece_input = Input(shape=(2, 4, 1), name='current_piece_input')\n",
        "  next_piece_input = Input(shape=(2, 4, 1), name='next_piece_input')\n",
        "  position_input = Input(shape=(2), name=\"position_input\")\n",
        "\n",
        "  # Create the CNN branch\n",
        "  board_branch = Conv2D(32, (3, 3), activation='relu')(board_input)\n",
        "  board_branch = MaxPooling2D((2, 2), padding='same')(board_branch)\n",
        "  board_branch = Flatten()(board_branch)\n",
        "\n",
        "  current_branch = Conv2D(32, (2, 2), activation='relu')(current_piece_input)\n",
        "  current_branch = MaxPooling2D((2, 2), padding='same')(current_branch)\n",
        "  current_branch = Flatten()(current_branch)\n",
        "\n",
        "  next_branch = Conv2D(32, (2, 2), activation='relu')(next_piece_input)\n",
        "  next_branch = MaxPooling2D((2, 2), padding='same')(next_branch)\n",
        "  next_branch = Flatten()(next_branch)\n",
        "\n",
        "  # Concatenate the 3 branches + position\n",
        "  concatenated_inputs = concatenate([board_branch,\n",
        "                                     current_branch,\n",
        "                                     next_branch,\n",
        "                                     position_input],\n",
        "                                    name='concatenated_inputs')\n",
        "\n",
        "  # Create the dense branch for the concatenated inputs\n",
        "  dense_branch = Dense(512, activation='relu')(concatenated_inputs)\n",
        "  dense_branch = Dense(256, activation='relu')(dense_branch)\n",
        "  dense_branch = Dense(64, activation='relu')(dense_branch)\n",
        "  dense_branch = Dense(32, activation='relu')(dense_branch)\n",
        "\n",
        "  # Create the output layer\n",
        "  output = Dense(6, activation='softmax', name='output')(dense_branch)\n",
        "\n",
        "  # Define the model with 4 inputs and one output\n",
        "  model = tf.keras.Model(inputs=[board_input,\n",
        "                                 current_piece_input,\n",
        "                                 next_piece_input,\n",
        "                                 position_input],\n",
        "                         outputs=output)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "fW9yNaju1nXJ"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the neural network model.\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QK4s1OztKJm4",
        "outputId": "748aaedd-6422-4475-c00b-2b6f1d1bda50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_201\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " board_input (InputLayer)    [(None, 21, 10, 1)]          0         []                            \n",
            "                                                                                                  \n",
            " current_piece_input (Input  [(None, 2, 4, 1)]            0         []                            \n",
            " Layer)                                                                                           \n",
            "                                                                                                  \n",
            " next_piece_input (InputLay  [(None, 2, 4, 1)]            0         []                            \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " conv2d_605 (Conv2D)         (None, 19, 8, 32)            320       ['board_input[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_606 (Conv2D)         (None, 1, 3, 32)             160       ['current_piece_input[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_607 (Conv2D)         (None, 1, 3, 32)             160       ['next_piece_input[0][0]']    \n",
            "                                                                                                  \n",
            " max_pooling2d_605 (MaxPool  (None, 10, 4, 32)            0         ['conv2d_605[0][0]']          \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_606 (MaxPool  (None, 1, 2, 32)             0         ['conv2d_606[0][0]']          \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_607 (MaxPool  (None, 1, 2, 32)             0         ['conv2d_607[0][0]']          \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " flatten_604 (Flatten)       (None, 1280)                 0         ['max_pooling2d_605[0][0]']   \n",
            "                                                                                                  \n",
            " flatten_605 (Flatten)       (None, 64)                   0         ['max_pooling2d_606[0][0]']   \n",
            "                                                                                                  \n",
            " flatten_606 (Flatten)       (None, 64)                   0         ['max_pooling2d_607[0][0]']   \n",
            "                                                                                                  \n",
            " position_input (InputLayer  [(None, 2)]                  0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " concatenated_inputs (Conca  (None, 1410)                 0         ['flatten_604[0][0]',         \n",
            " tenate)                                                             'flatten_605[0][0]',         \n",
            "                                                                     'flatten_606[0][0]',         \n",
            "                                                                     'position_input[0][0]']      \n",
            "                                                                                                  \n",
            " dense_802 (Dense)           (None, 512)                  722432    ['concatenated_inputs[0][0]'] \n",
            "                                                                                                  \n",
            " dense_803 (Dense)           (None, 256)                  131328    ['dense_802[0][0]']           \n",
            "                                                                                                  \n",
            " dense_804 (Dense)           (None, 64)                   16448     ['dense_803[0][0]']           \n",
            "                                                                                                  \n",
            " dense_805 (Dense)           (None, 32)                   2080      ['dense_804[0][0]']           \n",
            "                                                                                                  \n",
            " output (Dense)              (None, 6)                    198       ['dense_805[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 873126 (3.33 MB)\n",
            "Trainable params: 873126 (3.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Constants\n",
        "NUM_EPISODES = 5 # @param {type:\"integer\"}\n",
        "BATCH_SIZE = 31 # @param {type:\"integer\"}\n",
        "DISCOUNT_FACTOR = .99 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "EXPLORATION_EPSILON = .1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "LEARNING_RATE = 0.001 # @param {type:\"number\"}\n",
        "\n",
        "INVALID_MOVE_REWARD = -999 # @param {type:\"integer\"}\n",
        "GAME_OVER_REWARD = -999 # @param {type:\"integer\"}\n",
        "VALID_MOVE_REWARD = 1 # @param {type:\"integer\"}\n"
      ],
      "metadata": {
        "id": "TKAQEhkBwb5v"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and loss function based on your RL task.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)  # Adjust the learning rate as needed.\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()  # Adjust the loss function as needed.\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)"
      ],
      "metadata": {
        "id": "Y3Tgl3HHKJ7v"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]"
      ],
      "metadata": {
        "id": "num5EkHMNZ3u"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "def stateToModelInput(state):\n",
        "  return (tf.convert_to_tensor(np.reshape(state[\"board\"], (1, 21, 10,))),\n",
        "          tf.convert_to_tensor(np.reshape(state[\"current_piece\"], (1, 2, 4))),\n",
        "          tf.convert_to_tensor(np.reshape(state[\"next_piece\"], (1, 2, 4))),\n",
        "          tf.convert_to_tensor(np.reshape(state[\"position\"], (1, 2))))\n",
        "\n",
        "def dqn(env,\n",
        "        num_episodes,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        gamma=DISCOUNT_FACTOR,\n",
        "        epsilon=EXPLORATION_EPSILON,\n",
        "        learning_rate=LEARNING_RATE):\n",
        "    replay_buffer = ReplayMemory(max_size=10000)\n",
        "    model = create_model()  # Define the DQN neural network.\n",
        "    target_model = create_model()  # Target network for stability.\n",
        "\n",
        "    for episode in range(NUM_EPISODES):\n",
        "        env.start_game()\n",
        "        state = env.getState()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Epsilon-greedy action selection\n",
        "            if np.random.rand() < epsilon:\n",
        "                action = np.random.randint(6)  # Explore\n",
        "                #print(f\"Random Action: {action}\")\n",
        "            else:\n",
        "                action = np.argmax(model(stateToModelInput(state)))\n",
        "                #print(f\"Action: {action}\")\n",
        "\n",
        "            next_state, reward, done = env.doAction(action)\n",
        "            replay_buffer.append((state, action, reward, next_state, done))\n",
        "            state = next_state\n",
        "\n",
        "            # Sample and train on mini-batch from replay buffer\n",
        "            if replay_buffer.size >= batch_size:\n",
        "                mini_batch = replay_buffer.sample(batch_size)\n",
        "                train_dqn(model, target_model, mini_batch, gamma, learning_rate)\n",
        "\n",
        "        if episode % 10 == 0:\n",
        "            model.save(\"./model\")\n"
      ],
      "metadata": {
        "id": "fGJLD5DL543d"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_dqn(model, target_model, mini_batch, gamma, learning_rate):\n",
        "    # Step 1: Compute the target Q-values using the target model\n",
        "    states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
        "\n",
        "    target_q_values = []\n",
        "    for i, (next_state, done) in enumerate(zip(next_states, dones)):\n",
        "        if done:\n",
        "            target_q_values.append(rewards[i])  # If it's a terminal state, the Q-value is the immediate reward.\n",
        "        else:\n",
        "            # Use the target model to predict Q-values for the next state and select the maximum Q-value.\n",
        "            max_q_value = np.max(model(stateToModelInput(next_state))[0])\n",
        "            target_q = rewards[i] + gamma * max_q_value\n",
        "            target_q_values.append(target_q)\n",
        "\n",
        "    target_q_values = np.array(target_q_values)\n",
        "\n",
        "    # Step 2: Compute the predicted Q-values for the current states using the model\n",
        "    vec_stateToInput = np.vectorize(stateToModelInput)\n",
        "    model_inputs = list(map(np.vstack, zip(*map(stateToModelInput, states))))\n",
        "    # print(\"Curr pieces \", model_inputs[1].shape)\n",
        "    # print(\"Boards \", model_inputs[0].shape)\n",
        "    # print(\"Targets \", target_q_values.shape)\n",
        "\n",
        "    # Step 3: Calculate the target Q-values for the mini-batch\n",
        "    target_q_values = np.expand_dims(target_q_values, axis=1)  # Add an extra dimension for broadcasting\n",
        "\n",
        "    # Step 4: Compute the loss between the predicted Q-values and the target Q-values\n",
        "\n",
        "    #input()\n",
        "\n",
        "    # Step 5: Update the model's weights using gradient descent to minimize the loss\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    with tf.GradientTape() as tape:\n",
        "        predicted_q_values = model(model_inputs)\n",
        "        predicted_q_values = tf.reduce_sum(predicted_q_values * tf.one_hot(actions, 6), axis=1)\n",
        "        loss = tf.keras.losses.mean_squared_error(target_q_values, predicted_q_values)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss.numpy().mean()\n"
      ],
      "metadata": {
        "id": "3-EI4jNw_k7I"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tetris import TetrisApp\n",
        "\n",
        "REWARDS = (INVALID_MOVE_REWARD, GAME_OVER_REWARD, VALID_MOVE_REWARD)\n",
        "env = TetrisApp(ai=True, rewards=REWARDS)\n",
        "dqn(env,\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    gamma=DISCOUNT_FACTOR,\n",
        "    epsilon=EXPLORATION_EPSILON,\n",
        "    learning_rate=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "yUXUGuzI4V3S",
        "outputId": "7d509f6c-cec5-45ae-ad12-28469cc07a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        }
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-253-9cfe8df923cf>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mREWARDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mINVALID_MOVE_REWARD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAME_OVER_REWARD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALID_MOVE_REWARD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTetrisApp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREWARDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m dqn(env,\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-251-24b41723c2da>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(env, num_episodes, batch_size, gamma, epsilon, learning_rate)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tetris-ai/model/rl/tetris.py\u001b[0m in \u001b[0;36mstart_game\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mscoreChange\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdropped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscoreChange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrotate_stone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgameover\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tetris-ai/model/rl/tetris.py\u001b[0m in \u001b[0;36minitHuman\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_board\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_stone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pygame' is not defined"
          ]
        }
      ]
    }
  ]
}