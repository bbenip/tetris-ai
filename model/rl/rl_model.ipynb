{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbenip/tetris-ai/blob/main/model/rl/rl_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "MOUNT_DRIVE = True # @param {type:\"boolean\"}\n",
        "CLONE_REPO = False # @param {type:\"boolean\"}\n",
        "CONNECT_TPU = True # @param {type:\"boolean\"}\n"
      ],
      "metadata": {
        "id": "wFS3c7hZar0y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MOUNT_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  %cd drive/MyDrive"
      ],
      "metadata": {
        "id": "Hr08TYwna9Li",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "322b4b63-93cf-4199-d163-ccc2b1c2f24d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if CLONE_REPO:\n",
        "  !git clone https://github.com/bbenip/tetris-ai.git tetris-ai"
      ],
      "metadata": {
        "id": "TIvNEB8ob07F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  %cd tetris-ai/model/rl"
      ],
      "metadata": {
        "id": "ePArGlZYvQSL",
        "outputId": "b51c81b4-c690-4ba9-8e77-4c923da08568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/tetris-ai/model/rl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "28ZIpibPbTKq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d671b5f5-83ff-4ea7-d106-f386020713db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "L5O61oz5xoVB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if CONNECT_TPU:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  # This is the TPU initialization code that has to be at the beginning.\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "metadata": {
        "id": "qnSZJsvKxjMr",
        "outputId": "ba197868-7024-4ba9-d9f5-3e6764967a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.124.239.226:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Constants\n",
        "NUM_EPISODES = 1024 # @param {type:\"integer\"}\n",
        "MAX_MOVES = 1024 # @param {type:\"integer\"}\n",
        "BATCH_SIZE = 32 # @param {type:\"integer\"}\n",
        "DISCOUNT_FACTOR = 1 # @param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "EXPLORATION_START = 0.8 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "EXPLORATION_END = 0.3 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "E_DECAY_STEPS = 512 # @param {type:\"integer\"}\n",
        "LEARNING_RATE = 0.001 # @param {type:\"number\"}\n",
        "\n",
        "EPISODES_PER_UPDATE = 32 # @param {type:\"integer\"}\n",
        "\n",
        "INVALID_MOVE_REWARD = -999 # @param {type:\"integer\"}\n",
        "GAME_OVER_REWARD = -99 # @param {type:\"integer\"}\n",
        "VALID_MOVE_REWARD = 1 # @param {type:\"integer\"}\n",
        "\n",
        "#Amount subtracted from reward for every increase in line height\n",
        "LINE_HEIGHT_PENALTY = 10 # @param {type:\"integer\"}\n"
      ],
      "metadata": {
        "id": "TKAQEhkBwb5v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate, InputLayer"
      ],
      "metadata": {
        "id": "ELVzmmKfxsZZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  # Define the input for the CNN branch\n",
        "  board_input = Input(shape=(21, 10, 1), name='board_input')\n",
        "  current_piece_input = Input(shape=(4, 4, 1), name='current_piece_input')\n",
        "  next_piece_input = Input(shape=(4, 4, 1), name='next_piece_input')\n",
        "  position_input = Input(shape=(2), name=\"position_input\")\n",
        "  line_height_input = Input(shape=(1), name=\"line_height_input\")\n",
        "\n",
        "  # Create the CNN branch\n",
        "  board_branch = Conv2D(256, (3, 3), activation='relu')(board_input)\n",
        "  board_branch = MaxPooling2D((2, 2), padding='same')(board_branch)\n",
        "  board_branch = Conv2D(32, (3, 3), activation='relu')(board_branch)\n",
        "  board_branch = MaxPooling2D((2, 2), padding='same')(board_branch)\n",
        "  board_branch = Flatten()(board_branch)\n",
        "\n",
        "  current_branch = Flatten()(current_piece_input)\n",
        "\n",
        "  next_branch = Flatten()(next_piece_input)\n",
        "\n",
        "  # Concatenate the 3 branches + position\n",
        "  concatenated_inputs = concatenate([board_branch,\n",
        "                                     current_branch,\n",
        "                                     next_branch,\n",
        "                                     position_input,\n",
        "                                     line_height_input],\n",
        "                                    name='concatenated_inputs')\n",
        "\n",
        "  # Create the dense branch for the concatenated inputs\n",
        "  dense_branch = Dense(64, activation='relu')(concatenated_inputs)\n",
        "  dense_branch = Dense(32, activation='relu')(dense_branch)\n",
        "\n",
        "  # Create the output layer\n",
        "  output = Dense(6, activation='linear', name='output')(dense_branch)\n",
        "\n",
        "  # Define the model with 4 inputs and one output\n",
        "  model = tf.keras.Model(inputs=[board_input,\n",
        "                                 current_piece_input,\n",
        "                                 next_piece_input,\n",
        "                                 position_input,\n",
        "                                 line_height_input],\n",
        "                         outputs=output)\n",
        "\n",
        "  # Define the optimizer and loss function based on your RL task.\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)  # Adjust the learning rate as needed.\n",
        "  loss_fn = tf.keras.losses.MeanSquaredError()  # Adjust the loss function as needed.\n",
        "\n",
        "  # Compile the model.\n",
        "  model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "fW9yNaju1nXJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]"
      ],
      "metadata": {
        "id": "num5EkHMNZ3u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def getInputFromState(state):\n",
        "    model_input = state\n",
        "    model_input = model_input.write(0, tf.reshape(model_input.read(0), (1, 21, 10, 1)))\n",
        "    model_input = model_input.write(1, tf.expand_dims(tf.expand_dims(model_input.read(1), -1), 0))\n",
        "    model_input = model_input.write(2, tf.expand_dims(tf.expand_dims(model_input.read(2), -1), 0))\n",
        "    model_input.mark_used()\n",
        "    return model_input\n",
        "\n",
        "def dqn(env,\n",
        "        num_episodes,\n",
        "        pretrained_model=None,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        gamma=DISCOUNT_FACTOR,\n",
        "        ε_start=EXPLORATION_START,\n",
        "        ε_end=EXPLORATION_END,\n",
        "        ε_decay_steps=E_DECAY_STEPS,\n",
        "        learning_rate=LEARNING_RATE):\n",
        "    replay_buffer = ReplayMemory(max_size=10000)\n",
        "    model = pretrained_model if pretrained_model else create_model()  # Define the DQN neural network.\n",
        "    target_model =  pretrained_model if pretrained_model else create_model()  # Target network for stability.\n",
        "\n",
        "    current_step = 0\n",
        "    scores = np.zeros(NUM_EPISODES)\n",
        "    move_counts = np.zeros(NUM_EPISODES)\n",
        "    avg_past_ten = np.zeros(NUM_EPISODES // EPISODES_PER_UPDATE + 1)\n",
        "\n",
        "    for episode in range(NUM_EPISODES):\n",
        "        score = 0\n",
        "        moves = 0\n",
        "        count = 0\n",
        "\n",
        "        env.start_game()\n",
        "        state = getInputFromState(env.getState())\n",
        "        done = False\n",
        "        while not done and moves < MAX_MOVES:\n",
        "          if current_step < ε_decay_steps:\n",
        "            # Update epsilon using linear decay\n",
        "            epsilon = max(ε_end, ε_start - (ε_start - ε_end) * (current_step / ε_decay_steps))\n",
        "\n",
        "          # Epsilon-greedy action selection\n",
        "          if np.random.rand() < epsilon:\n",
        "              action = np.random.randint(6)  # Explore\n",
        "              #print(f\"Random Action: {action}\")\n",
        "          else:\n",
        "              action = np.argmax(model(state))\n",
        "              #print(f\"Action: {action}\")\n",
        "\n",
        "          next_state, reward, done = env.doAction(action)\n",
        "          next_state = getInputFromState(next_state)\n",
        "          score += reward if reward > 0 else 0\n",
        "          replay_buffer.append((state, action, reward, next_state, done))\n",
        "          state = next_state\n",
        "\n",
        "          count += 1\n",
        "          moves += 1\n",
        "          # Sample and train on mini-batch from replay buffer\n",
        "          if replay_buffer.size >= batch_size and count > batch_size:\n",
        "              count = 0\n",
        "              mini_batch = replay_buffer.sample(batch_size)\n",
        "              train_dqn(model, target_model, mini_batch, gamma, learning_rate)\n",
        "        current_step += 1\n",
        "        scores[episode] = score\n",
        "        move_counts[episode] = moves\n",
        "        avg_past_ten[episode // EPISODES_PER_UPDATE] = np.average(scores[episode-10:episode])\n",
        "        if episode and episode % EPISODES_PER_UPDATE == 0:\n",
        "            #clear_output(wait=True)\n",
        "            print(f\"Episode: {episode} \\nAverage Score (Past 10 games): {avg_past_ten[episode // EPISODES_PER_UPDATE]}\")\n",
        "            fig, ax1 = plt.subplots()\n",
        "\n",
        "            color = 'tab:red'\n",
        "            ax1.set_xlabel('Episode')\n",
        "            ax1.set_ylabel('Score', color=color)\n",
        "            ax1.scatter(range(episode), scores[:episode], color=color)\n",
        "            ax1.plot(range(0, episode, EPISODES_PER_UPDATE), avg_past_ten[:episode // EPISODES_PER_UPDATE], color=\"black\", linewidth=2, label='moving_avg')\n",
        "            ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "            fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "            plt.show()\n",
        "\n",
        "        if episode % (EPISODES_PER_UPDATE * 2) == 0:\n",
        "            model.save(\"./model\")\n"
      ],
      "metadata": {
        "id": "fGJLD5DL543d"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_dqn(model, target_model, mini_batch, gamma, learning_rate):\n",
        "    # Step 1: Compute the target Q-values using the target model\n",
        "    states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
        "\n",
        "    states_iter = zip(next_states, rewards, dones)\n",
        "    target_q_values = []\n",
        "    for next_state, reward, done in states_iter:\n",
        "        if done:\n",
        "            target_q_values.append(reward)  # If it's a terminal state, the Q-value is the immediate reward.\n",
        "        else:\n",
        "            # Use the target model to predict Q-values for the next state and select the maximum Q-value.\n",
        "            max_q_value = np.max(model(next_state)[0])\n",
        "            target_q = reward + gamma * max_q_value\n",
        "            target_q_values.append(target_q)\n",
        "\n",
        "    target_q_values = np.array(target_q_values)\n",
        "\n",
        "    # Step 2: Compute the predicted Q-values for the current states using the model\n",
        "    model_inputs = []\n",
        "    for ins in zip(*states):\n",
        "      model_inputs.append(tf.concat(ins, 0))\n",
        "    # print(\"Curr pieces \", model_inputs[1].shape)\n",
        "    # print(\"Boards \", model_inputs[0].shape)\n",
        "    # print(\"Targets \", target_q_values.shape)\n",
        "\n",
        "    # Step 3: Calculate the target Q-values for the mini-batch\n",
        "    target_q_values = np.expand_dims(target_q_values, axis=1)  # Add an extra dimension for broadcasting\n",
        "\n",
        "    # Step 5: Update the model's weights using gradient descent to minimize the loss\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    with tf.GradientTape() as tape:\n",
        "      #predicted_q_values = tf.reduce_sum(tf.multiply(predicted_q_values, masks), axis=1)\n",
        "      tape.watch(model_inputs)\n",
        "      predicted_q_values = model(model_inputs)\n",
        "      predicted_q_values = tf.map_fn(tf.reduce_max, predicted_q_values)\n",
        "      loss = tf.keras.losses.mean_squared_error(target_q_values, predicted_q_values)\n",
        "      gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss.numpy().mean()\n"
      ],
      "metadata": {
        "id": "3-EI4jNw_k7I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_model = None"
      ],
      "metadata": {
        "id": "gflapR-uw0N0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_model = tf.keras.models.load_model(\"./model\")"
      ],
      "metadata": {
        "id": "feMekBrmvjY2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "6a7a410f-557a-4002-ea9f-66699512efda"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-59821eeb18e0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpre_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    231\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at ./model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tetris import TetrisApp\n",
        "\n",
        "REWARDS = (INVALID_MOVE_REWARD, GAME_OVER_REWARD, VALID_MOVE_REWARD, LINE_HEIGHT_PENALTY)\n",
        "env = TetrisApp(ai=True, rewards=REWARDS)\n",
        "dqn(env,\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    pretrained_model=pre_model,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    gamma=DISCOUNT_FACTOR,\n",
        "    learning_rate=LEARNING_RATE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "yUXUGuzI4V3S",
        "outputId": "affacb9b-95c1-4519-93b6-1a6d554d44dd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function check_collision at 0x7d332ae01b40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function join_matrices at 0x7d326b8f32e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function check_collision at 0x7d332ae01b40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function join_matrices at 0x7d326b8f32e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-8377f5ade845>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mREWARDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mINVALID_MOVE_REWARD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAME_OVER_REWARD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVALID_MOVE_REWARD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLINE_HEIGHT_PENALTY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTetrisApp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREWARDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m dqn(env,\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpretrained_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-3a2a63c0be3b>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(env, num_episodes, pretrained_model, batch_size, gamma, ε_start, ε_end, ε_decay_steps, learning_rate)\u001b[0m\n\u001b[1;32m     48\u001b[0m               \u001b[0;31m#print(f\"Random Action: {action}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m               \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m               \u001b[0;31m#print(f\"Action: {action}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# which does not have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;34mf\"Inputs to a layer should be tensors. Got '{x}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;34mf\"(of type {type(x)}) as input for layer '{layer_name}'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got '<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7d3268ebf6a0>' (of type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>) as input for layer 'model'."
          ]
        }
      ]
    }
  ]
}